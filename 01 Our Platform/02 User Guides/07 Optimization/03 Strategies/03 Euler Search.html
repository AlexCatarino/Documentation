<div>What is Euler search:</div><div>-<br></div><div><br></div><div>Why use Eluer search (pros):</div><div>-<br></div><div><br></div><div>Why not to use Eluer search (cons):</div><div>-<br></div><div><br></div><div>----------------<br></div><div>&lt;h3&gt;Random search</div><div><div>What is random search:</div><div>-You provide a statistical distribution of each parameter and the number of samples. The distributions of each parameter are randomly sampled to determine parameter values to test.<br></div><div><br></div><div>Why use random search (pros):</div><div>-Random search can find parameter combinations that result in a better value for the objective function when compared to Grid search. This is because a large step size on Grid search can cause the grid to skip over the most important parameter value.</div><div>-The random search algorithm can explore areas the Grid search may skip over (if the step size causes the algorithm to skip over values)</div><div>-"Embarrassingly parallel": tests are independent of eachother so you can run concurrent backtests when optimizing<br></div><div><br></div><div>Why not to use random search (cons):</div>-Results may not be repeatable (if you don't set a random seed)</div><div>-The results may be hard to visualize (heatmap result axis may skip over values)</div><div>-Not currently supported by the optimizer<br></div><div><br></div><div>-------------------------</div><div>&lt;h3&gt;Gradient descent optimization <br></div><div>What is gradient descent:</div><div>(1) Values for each parameter are first selected (can be random) and the objective function is evaluated<br></div><div>(2) The gradient of the objective function is computed for neighboring parameter values</div><div>(3) The search algorithm steps in the direction which improves the result of the objective function</div><div>(4) The process continues steps 1-3 until the result of the objective function can no longer improve<br></div><div><br></div><div>Why use gradient descent (pros):</div><div>-Customizable <br></div><div>&nbsp;&nbsp;&nbsp; - you can select multiple samples from the global search space before selecting one of the samplings to perform optimization</div><div>&nbsp;&nbsp;&nbsp; - You can specify the step size / learning rate</div><div>-Repeatable results (if there is no randomization included)<br></div><div><br></div><div>Why not to use gradient descent (cons):</div><div>-The search algorithm can get stuck in local optima<br></div><div>-Not currently supported by the optimizer</div><div><br></div><div>-------------------------</div><div>&lt;h3&gt;Evolutionary optimization</div><div><div>What is evolutionary optimization:</div><div>-Inspired by the biological process of evolution</div><div>-(1) Start with a set of random solutions (random values for each parameter)</div><div>-(2) Evaluate the performance of those sets of parameter values using the objective function<br></div><div>-(3) Select the best performing sets of parameter values and use crossover and mutation to replace the worst-performing sets of parameter values (form the next generation)<br></div><div>-(4) Repeat steps 2-3 until either:</div><div>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - The objective function passes a threshold</div><div>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - The value of the objective function stops improving</div><div>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - A pre-defined number of generations have been formed<br></div><div><br></div><div><br></div><div>Why use evolutionary optimization (pros):</div><div>-They generally perform well on a large variety of problems</div><div><br></div><div>Why not to use evolutionary optimization (cons):</div><div>- The randomization can make results not repeatable (unless a random seed is set)</div><div>- Since generations are formed, it's not embarrassingly parallel. So it may take longer to perform this type of optimization.<br></div><div>- Not currently supported by the optimizer</div></div><div></div>