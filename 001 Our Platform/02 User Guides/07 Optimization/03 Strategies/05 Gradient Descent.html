<div>What is gradient descent:</div><div>(1) Values for each parameter are first selected (can be random) and the objective function is evaluated<br></div><div>(2) The gradient of the objective function is computed for neighboring parameter values</div><div>(3) The search algorithm steps in the direction which improves the result of the objective function</div><div>(4) The process continues steps 1-3 until the result of the objective function can no longer improve<br></div><div><br></div><div>Why use gradient descent (pros):</div><div>-Customizable <br></div><div>&nbsp;&nbsp;&nbsp; - you can select multiple samples from the global search space before selecting one of the samplings to perform optimization</div><div>&nbsp;&nbsp;&nbsp; - You can specify the step size / learning rate</div><div>-Repeatable results (if there is no randomization included)<br></div><div><br></div><div>Why not to use gradient descent (cons):</div><div>-The search algorithm can get stuck in local optima<br></div><div>-Not currently supported by the optimizer</div><div></div>