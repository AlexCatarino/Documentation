<p>
  <a rel="nofollow" target="_blank" href='https://docs.python.org/3/library/profile.html'>Python Profilers</a> from the Python Standard Library enable you to find the functions and methods in your algorithm that consume most of the runtime.
  To analyze your algorithm's efficiency with the Python Profilers, follow these steps:
</p>

<ol>
  <li>Open the project in the <a href='/docs/v2/cloud-platform/projects/getting-started#02-View-All-Projects'>Cloud Platform</a>, <a href='/docs/v2/local-platform/projects/getting-started#04-Open-Projects'>Local Platform</a>, or a local IDE if you use the CLI.</p>

  <li>At the top of the <span class='public-file-name'>main.py</span> file, import the classes you need and enable the <code>Profile</code>.</li>
  <div class="section-example-container">
    <pre class="python">from cProfile import Profile
from pstats import Stats
from io import StringIO

# Create the profile and start collecting profiling data.
profile = Profile()
profile.enable()</pre>
</div>

  <li>Define or extend the <a href='/docs/v2/writing-algorithms/key-concepts/event-handlers#15-End-Of-Algorithm-Events'>on_end_of_algorithm</a> method to <code>disable</code> the <code>Profile</code> and save information on the most time-consuming functions to the <a href='/docs/v2/research-environment/object-store'>Object Store</a>.</li>

  <div class="section-example-container">
    <pre class="python">class FunctionTimeConsumptionAlgorithm(QCAlgorithm):
    def initialize(self):
        pass

    def on_end_of_algorithm(self):
        # Stop collecting profiling data
        profile.disable()
        stream = StringIO()
        # Save the top 20 time-consuming functions to a file in the Object Store.
        Stats(profile, stream=stream).sort_stats('cumulative').print_stats(20)
        # Save the profiling data to the Object Store using the algorithm ID.
        self.object_store.save(f"{self.algorithm_id}_profile", stream.getvalue())</pre>
  </div>

  <li><a href='/docs/v2/research-environment/key-concepts/getting-started#03-Open-Notebooks'>Open the Research Environment</a> and create a <code>QuantBook</code>.</li>
  <div class="section-example-container">
    <pre class="python">qb = QuantBook()</pre>
  </div>

  <li>Get the backtest Id and read the profiling data from the Object Store.</li>
  <p>The process to get the backtest Id depends on if you use the <a href='/docs/v2/cloud-platform/backtesting/getting-started#06-Get-Backtest-Id'>Cloud Platform</a>, <a href='/docs/v2/local-platform/backtesting/getting-started#07-Get-Backtest-Id'>Local Platform</a>, or <a href='/docs/v2/lean-cli/backtesting/deployment#05-Get-Backtest-Id'>CLI</a>.</p>
  <div class="section-example-container">
    <pre class="python">backtest_id = "8b16cec0c44f75188d82f9eadb310e17"
profile_output = qb.object_store.read(f"{backtest_id}_profile")
print(profile_output)</pre>
  </div>
</ol>

<p>
  To increase the speed of some methods, you can use decorators like <code>@jit</code> or <code>@lru_cache</code>.
  The <code>@jit</code> decorator from Numba is for Just-In-Time (JIT) compilation.
  It compiles Python code to machine code at runtime to increase the speed of loops and mathematical operations.
  However, Numba can't compile all Python code.
  It performs best with NumPy arrays.
  If you add the <code>@jit</code> decorator to your methods, it can make the debugging process more challenging since the code is compiled to machine code.
  The following code snippet shows an example of using the <code>@jit</code> decorator:
</p>

<div class="section-example-container">
<pre class="python">import numpy as np
from numba import jit
import time

# Without JIT
def slow_function(arr):
    result = 0
    for i in range(len(arr)):
        result += np.sin(arr[i]) * np.cos(arr[i])
    return result

# With JIT
@jit(nopython=True)
def fast_function(arr):
    result = 0
    for i in range(len(arr)):
        result += np.sin(arr[i]) * np.cos(arr[i])
    return result

# Example usage
arr = np.random.rand(1000000) 

# Test without JIT.
start = time.time()
slow_function(arr)
print(f"Slow function took: {time.time() - start} seconds")

# Test with JIT.
start = time.time()
fast_function(arr)  # This will compile first, then run.
print(f"Fast function took: {time.time() - start} seconds")</pre>
</div>

<blockquote>
<p>Slow function took: 1.1557221412658691 seconds</p>
<p>Fast function took: 0.32973551750183105 seconds</p>
</blockquote>
<br>

<p>
  The <code>@lru_cache</code> decorator from the <code>functools</code> module is for Least Recently Used (LRU) caching.
  It caches the result of a function based on its arguments, so that the function body doesn't have to repeatedly execute when you call the function multiple times with the same inputs.
  The <code>maxsize</code> argument of the decorator defines the number of results to cache.
  For example, <code>@lru_cache(maxsize=512)</code> means that it should cache a maximum of 512 results.
  If you add the <code>@lru_cache</code> decorator to your methods, make sure your function is completely dependent on the arguments (not any global state variables) or else the caching process may lead to incorrect results.
  The following code snippet shows an example of using the <code>@lru_cache</code> decorator:
</p>

<div class="section-example-container">
<pre class="python">from functools import lru_cache
import time

# Without caching
def fibonacci(n):
    if n < 2:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

# With caching
@lru_cache(maxsize=512)
def fibonacci_cached(n):
    if n < 2:
        return n
    return fibonacci_cached(n-1) + fibonacci_cached(n-2)

n = 30

# Test without caching.
start = time.time()
for _ in range(n):
    fibonacci(n)
print(f"Fibonacci without cache took: {time.time() - start} seconds")

# Test with caching.
start = time.time()
for _ in range(n):
    fibonacci_cached(n)
print(f"Fibonacci with cache took: {time.time() - start} seconds")</pre>
</div>

<blockquote>
<p>Fibonacci without cache took: 4.387492418289185 seconds</p>
<p>Fibonacci with cache took: 9.322166442871094e-05 seconds</p>
</blockquote>

