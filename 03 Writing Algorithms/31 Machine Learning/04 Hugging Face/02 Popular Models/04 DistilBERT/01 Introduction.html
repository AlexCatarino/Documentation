<p>This page explains how to use DistilBERT in LEAN trading algorithms. The <a href='https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad' rel='nofollow' target='_blank'>model repository</a> provides the following description:</p>

<blockquote cite="https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad">
<p>
  The DistilBERT model was proposed in the blog post <a href='https://medium.com/huggingface/distilbert-8cf3380435b5' rel='nofollow' target='_blank'>Smaller, faster, cheaper, lighter: Introducing DistilBERT, adistilled version of BERT</a>, and the paper <a href='https://arxiv.org/abs/1910.01108' rel='nofollow' target='_blank'>DistilBERT, adistilled version of BERT: smaller, faster, cheaper and lighter</a>. 
  DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark.

  This model is a fine-tune checkpoint of <a href='https://huggingface.co/distilbert-base-cased' rel='nofollow' target='_blank'>DistilBERT-base-cased</a>, fine-tuned using (a second step of) knowledge distillation on <a href='https://huggingface.co/datasets/squad'>SQuAD v1.1</a>.
</p>
</blockquote>

