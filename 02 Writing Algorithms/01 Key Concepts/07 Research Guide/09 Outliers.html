<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<p>Outliers in a dataset can have large impacts on how models train. In some cases, you may leave outliers in a dataset because they can contain useful information. In other cases, you may want to transform the data to handler the outlier data points. There are several common methods to handle outliers.</p>

<h4>Winsorization Method</h4>
<p>The winsorization method removes outliers at the \(c_i\) testing</p>

$\[P(c_i | x_1, ..., x_n) = \frac{P(c_i)P(x_1, ..., x_n | c_i)}{P(x_1, ..., x_n)}\]$

<br>
\[P(c_i | x_1, ..., x_n) = \frac{P(c_i)P(x_1, ..., x_n | c_i)}{P(x_1, ..., x_n)}\]

<h4>IQR Method</h4>

<h4>Factor Ranking Method</h4>



How are outliers usually handled:<br>&nbsp;- Leaving them in the data (outliers may contain useful information)<br>&nbsp;- Trimming them out by winsorization or IQR truncation<br>&nbsp;- Using factor ranks instead of raw factor values<br><br>-Winsorization method removes outliers in the x% of the extremes<br>&nbsp;&nbsp; - Often subjective<br>&nbsp;&nbsp; - The percentile can change over time or based on the application.<br>&nbsp;&nbsp; - Can result in overfitting<br><br><div>-IQR method</div><div>&nbsp;&nbsp; - Outliers are identified if they fall outside of the range [ Q1− k( Q3 − Q1 ), Q3 + k(Q3 − Q1) ] for some constant k &gt;= 0.</div><div>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - This method can exclude up to 25% of observations on each side of the extremes<br></div>&nbsp;&nbsp; - Doesn't work if the data is skewed or non-normal. Therefore, the downside is the you need to review the factor distribution properties. <br><div>&nbsp;&nbsp; - If you need to, you can normalize the data with z-scores.</div><div>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - &lt;add z-score formula&gt;<br></div><br><div>-Factor ranking method</div><div>&nbsp;&nbsp; - Taking the rank of the factor relative to the other securities, rather than taking the raw factor value.<br></div>&nbsp;&nbsp; - This method transforms the data into a uniform distribution.<br>&nbsp;&nbsp; - After converting to a uniform distribution, Wang et al. perform an inverse normal transformation to make the factors normally distributed.<br><br>-Wang et al. research finds that the ranking transformation outperforms the z-score transformation, suggesting that the distance between factor scores doesn't add information (just noise), only the ranking matters.<p></p>
